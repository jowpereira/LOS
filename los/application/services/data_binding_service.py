"""
ğŸ”— Data Binding Service - Conecta dados reais aos parÃ¢metros do modelo
D01-D04: ValidaÃ§Ã£o e mapeamento de DataFrames/dicts para parÃ¢metros AST.
"""

from typing import Any, Dict, List, Optional, Union
from pathlib import Path
import pandas as pd
import numpy as np

from ...shared.errors.exceptions import ValidationError
from ...shared.logging.logger import get_logger

_logger = get_logger(__name__)


class DataBindingService:
    """
    ServiÃ§o responsÃ¡vel por validar e preparar dados de entrada para o modelo.
    Garante que os dados fornecidos (data) correspondem Ã  estrutura esperada pelos parÃ¢metros (ast).
    """

    def bind_data(self, ast: Dict[str, Any], data: Optional[Dict[str, Any]] = None, base_dir: Optional[Path] = None) -> Dict[str, Any]:
        """
        Valida e prepara os dados para injeÃ§Ã£o no modelo.

        Args:
            ast: Ãrvore sintÃ¡tica do modelo
            data: Dados explÃ­citos (override)
            base_dir: DiretÃ³rio base do modelo para resolver imports
        """
        # S01: Separate input sources (DataFrames) from bound output (Lists/Dicts)
        # to prevents overwriting source DFs when extracting Sets.
        input_sources = {}
        
        # 1. Carregar imports (baixa prioridade)
        if base_dir:
             imported_data = self._load_imports(ast, base_dir)
             input_sources.update(imported_data)

        # 2. Carregar dados explÃ­citos (alta prioridade - override)
        if data:
            input_sources.update(data)
            
        bound_data = {}

        # Bind Sets
        sets = self._extract_sets(ast)
        for set_name, set_def in sets.items():
            vals = None
            # 1. Direct match (file name == set name) in input_sources
            if set_name in input_sources:
                data_val = input_sources[set_name]
                if isinstance(data_val, pd.DataFrame):
                    if set_name in data_val.columns:
                        vals = data_val[set_name].dropna().unique().tolist()
                    elif data_val.index.name == set_name:
                         vals = data_val.index.unique().tolist()
                    else:
                        vals = data_val.iloc[:, 0].dropna().unique().tolist()
                elif isinstance(data_val, pd.Series):
                    vals = data_val.unique().tolist()
                elif isinstance(data_val, (set, tuple, list)):
                    vals = list(data_val)
            
            # 2. D03: Search in other DataFrames within input_sources
            if not vals:
                 for key, val in input_sources.items():
                    if isinstance(val, pd.DataFrame) and set_name in val.columns:
                        vals = val[set_name].dropna().unique().tolist()
                        _logger.debug(f"Set '{set_name}' encontrado no DataFrame '{key}'")
                        break
            
            if vals:
                bound_data[set_name] = vals

        parameters = self._extract_parameters(ast)

        for param_name, param_def in parameters.items():
            # Check input_sources first
            if param_name in input_sources:
                raw_value = input_sources[param_name]
                validated_value = self._validate_and_transform(param_name, param_def, raw_value, bound_data)
                bound_data[param_name] = validated_value
            else:
                # D03: Tentar encontrar o parÃ¢metro como coluna em outros DataFrames importados (input_sources)
                found_in_df = False
                for key, val in input_sources.items():
                    # DEBUG PRINT
                    # print(f"Checking DF '{key}' for param '{param_name}'. Columns: {val.columns if isinstance(val, pd.DataFrame) else 'Not DF'}")
                    if isinstance(val, pd.DataFrame) and param_name in val.columns:
                        try:
                            _logger.debug(f"ParÃ¢metro '{param_name}' encontrado no DataFrame '{key}'")
                            # Copia o DataFrame para evitar efeitos colaterais
                            # O _validate_and_transform vai lidar com set_index e extraÃ§Ã£o
                            validated_value = self._validate_and_transform(param_name, param_def, val.copy(), bound_data)
                            bound_data[param_name] = validated_value
                            found_in_df = True
                            break
                        except Exception as e:
                            _logger.warning(f"Falha ao extrair '{param_name}' do DataFrame '{key}': {e}")
                
                if not found_in_df:
                    _logger.warning(f"ParÃ¢metro '{param_name}' nÃ£o encontrado nos dados importados. UsarÃ¡ valor padrÃ£o. DICA: Verifique se o nome da coluna no CSV corresponde exatamente ao nome do parÃ¢metro.")

        return bound_data

    def _load_imports(self, ast: Dict[str, Any], base_dir: Path) -> Dict[str, Any]:
        """LÃª arquivos importados no AST (ex: import "file.csv")"""
        loaded = {}
        for stmt in ast.get('statements', []):
            if stmt.get('type') == 'import':
                path_str = stmt.get('path')
                if not path_str: continue
                
                safe_path = Path(path_str)
                # Resolve relative path
                full_path = base_dir / safe_path
                
                if full_path.exists() and full_path.suffix.lower() == '.csv':
                    try:
                        # Assume filename stem is the variable name (e.g. demanda.csv -> demanda)
                        var_name = safe_path.stem
                        _logger.info(f"Carregando import: {var_name} de {full_path}")
                        loaded[var_name] = pd.read_csv(full_path)
                    except Exception as e:
                        _logger.warning(f"Falha ao carregar import {full_path}: {e}")
        return loaded

    def _extract_parameters(self, ast: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai definiÃ§Ãµes de parÃ¢metros da AST."""
        params = {}
        # AST estrutura: {'statements': [...]}
        for stmt in ast.get('statements', []):
            if stmt.get('type') == 'param':
                name = stmt['name']
                params[name] = stmt
        return params

    def _validate_and_transform(self, name: str, definition: Dict[str, Any], value: Any, context: Dict[str, Any] = None) -> Any:
        """
        Valida se o valor corresponde Ã  definiÃ§Ã£o do parÃ¢metro e transforma se necessÃ¡rio.
        Args:
            context: bound_data atual (para acesso a Sets)
        """
        indices = definition.get('indices')
        
        # Caso 1: Escalar (sem Ã­ndices)
        if not indices:
            if isinstance(value, (pd.DataFrame, pd.Series, dict, list)):
                 raise ValidationError(f"ParÃ¢metro '{name}' Ã© escalar, mas recebeu dados estruturados.")
            try:
                return float(value)
            except (ValueError, TypeError):
                raise ValidationError(f"ParÃ¢metro '{name}' espera valor numÃ©rico, recebeu {type(value)}")

        # Caso 2: Indexado (Array/Matriz)
        if isinstance(value, pd.DataFrame):
            return self._process_dataframe(name, indices, value, context)
        elif isinstance(value, pd.Series):
            return self._process_series(name, indices, value)
        elif isinstance(value, dict):
            return self._process_dict(name, indices, value)
        else:
            raise ValidationError(f"ParÃ¢metro '{name}' indexado espera DataFrame/Series/dict, recebeu {type(value)}")

    def _process_dataframe(self, name: str, indices: List[str], df: pd.DataFrame, context: Dict[str, Any] = None) -> Dict:
        """Transforma DataFrame em dicionÃ¡rio aninhado, garantindo densidade se possÃ­vel."""
        expected_levels = len(indices)
        
        # Tenta setar Ã­ndice se as colunas existirem
        available_cols = set(df.columns)
        indices_to_set = [idx for idx in indices if idx in available_cols]
        
        if len(indices_to_set) > 0:
            # Se encontrou todos ou alguns, seta.
            # Se for parcial, depois verifica levels.
            try:
                df = df.set_index(indices_to_set)
            except Exception as e:
                _logger.warning(f"Falha ao definir Ã­ndice {indices_to_set} para parÃ¢metro '{name}': {e}. Usando Ã­ndice padrÃ£o.")
        
        # Seleciona a coluna de valor
        value_col = None
        if name in df.columns:
            value_col = name
        elif len(df.columns) == 1:
            value_col = df.columns[0]
        else:
            # Fallback: primeira coluna numÃ©rica? ou error
            # Tenta achar 'value', 'valor'
            for c in ['value', 'valor', 'val']:
                if c in df.columns:
                    value_col = c
                    break
            if not value_col:
                # Last resort: first column that is not index
                value_col = df.columns[0]
                _logger.warning(f"Aviso: Inferindo coluna '{value_col}' como valor para parÃ¢metro '{name}'. Se incorreto, renomeie a coluna no CSV.")

        series = df[value_col]

        # Auto-Densification Logic
        if context:
            # Verifica se podemos reconstruir o Ã­ndice completo
            # Precisamos que TODOS os Ã­ndices estejam no context (como Sets/Lists)
            can_reindex = True
            levels = []
            for idx_name in indices:
                if idx_name in context:
                    # Assume que Ã© uma lista de ids
                    vals = context[idx_name]
                    if isinstance(vals, (list, tuple, set)):
                         levels.append(list(vals))
                    else:
                        can_reindex = False
                        break
                else:
                    can_reindex = False
                    break
            
            if can_reindex and len(levels) == len(indices):
                # S06: Fix 1-level MultiIndex issue.
                # MultiIndex.from_product creates tuples even for 1 level. Use Index for 1 level.
                if len(indices) == 1:
                    full_idx = pd.Index(levels[0], name=indices[0])
                else:
                    full_idx = pd.MultiIndex.from_product(levels, names=indices)
                
                # D05: Heuristic - If the source DF has NO overlap with the target index, it's likely the wrong DF.
                # Unless the target index is empty (which shouldn't happen here).
                if not full_idx.empty:
                    # Normalize indices for comparison (types might differ, e.g. str vs int)
                    # But loose intersection is safer.
                    intersection = series.index.intersection(full_idx)
                    if intersection.empty:
                            raise ValueError(f"DataFrame source has no overlap with target indices {indices}. Skipping.")

                try:
                    # Reindex com fill_value=0 (AssunÃ§Ã£o: default=0)
                    # TODO: Pegar default do AST se possÃ­vel
                    series = series.reindex(full_idx, fill_value=0)
                    _logger.debug(f"Densificado parÃ¢metro '{name}' com {len(series)} entradas.")
                except Exception as e:
                    _logger.warning(f"Falha na densificaÃ§Ã£o automÃ¡tica de '{name}': {e}")
        
        # DEBUG PRINT
        # print(f"Processing DF for '{name}'. Series:\n{series}\nNested Dict:\n{self._to_nested_dict(series)}")
        return self._to_nested_dict(series)

    def _process_series(self, name: str, indices: List[str], series: pd.Series) -> Dict:
        """Processa Series para dict aninhado."""
        if series.index.nlevels != len(indices):
             raise ValidationError(f"ParÃ¢metro '{name}' espera {len(indices)} Ã­ndices, Series tem {series.index.nlevels}.")
        
        return self._to_nested_dict(series)

    def _process_dict(self, name: str, indices: List[str], data: Dict) -> Dict:
        """Valida dict. Assume que jÃ¡ estÃ¡ no formato correto (aninhado ou tuple keys?)."""
        # Se for tuple keys {(i,j): val}, converter para aninhado?
        # O Translator gera acesso `param[i][j]`.
        # Se o dict for {(i,j): val}, `param[i]` falha.
        # TEM QUE SER ANINHADO.
        
        if not data:
            return {}
            
        sample_key = next(iter(data))
        if isinstance(sample_key, tuple):
            # Converter flat dict {(i,j): v} -> nested {i: {j: v}}
            return self._unflatten_dict(data)
        
        return data

    def _to_nested_dict(self, series: pd.Series) -> Dict:
        """Converte pandas Series (com MultiIndex) para dict aninhado."""
        if series.index.nlevels == 1:
            return series.to_dict()
            
        # Para MultiIndex, Ã© mais chato.
        # Ex: (A, B) -> val
        # {A: {B: val}}
        
        # Groupby no primeiro nÃ­vel e recursÃ£o? Lento.
        # Iterar? Lento.
        
        # Melhor abordagem:
        # Loop sobre o Ã­ndice.
        d = {}
        for idx, val in series.items():
            if not isinstance(idx, tuple):
                idx = (idx,)
            
            current = d
            for i in idx[:-1]:
                if i not in current:
                    current[i] = {}
                current = current[i]
            current[idx[-1]] = val
            
        return d

    def _unflatten_dict(self, flat_dict: Dict) -> Dict:
        """Converte {(i,j): val} para {i: {j: val}}."""
        d = {}
        for idx, val in flat_dict.items():
            if not isinstance(idx, tuple):
                d[idx] = val # Should not happen if detected as tuple
                continue
                
            current = d
            for i in idx[:-1]:
                if i not in current:
                    current[i] = {}
                current = current[i]
            current[idx[-1]] = val
        return d

    def _extract_sets(self, ast: Dict[str, Any]) -> Dict[str, Any]:
        """Extrai definiÃ§Ãµes de conjuntos (sets) da AST."""
        sets = {}
        for stmt in ast.get('statements', []):
            if stmt.get('type') == 'set':
                name = stmt['name']
                sets[name] = stmt
        return sets
